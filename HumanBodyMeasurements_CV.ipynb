{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshbuxyyes/Human-Body-Measurements-using-Computer-Vision/blob/master/HumanBodyMeasurements_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/farazBhatti/Human-Body-Measurements-using-Computer-Vision'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t4H0N9rGJgN",
        "outputId": "13ed4b25-218c-4ce4-f4e2-b81820ec2822"
      },
      "id": "6t4H0N9rGJgN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Human-Body-Measurements-using-Computer-Vision'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 215 (delta 67), reused 88 (delta 35), pack-reused 87\u001b[K\n",
            "Receiving objects: 100% (215/215), 2.34 MiB | 19.48 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change Python version\n"
      ],
      "metadata": {
        "id": "9BmdQtWvuYAo"
      },
      "id": "9BmdQtWvuYAo"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!sudo apt-get install python3.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNFnQyd8uXqR",
        "outputId": "2fc89f10-b100-466d-a10b-6640ba53070d"
      },
      "id": "LNFnQyd8uXqR",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libcasa-python3-6' for regex 'python3.6'\n",
            "Note, selecting 'libpython3.6-stdlib' for regex 'python3.6'\n",
            "Note, selecting 'python3.6-2to3' for regex 'python3.6'\n",
            "The following additional packages will be installed:\n",
            "  libcasa-casa6\n",
            "The following NEW packages will be installed:\n",
            "  libcasa-casa6 libcasa-python3-6\n",
            "0 upgraded, 2 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 1,088 kB of archives.\n",
            "After this operation, 4,238 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcasa-casa6 amd64 3.4.0-2build1 [1,000 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcasa-python3-6 amd64 3.4.0-2build1 [88.2 kB]\n",
            "Fetched 1,088 kB in 0s (6,677 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcasa-casa6:amd64.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../libcasa-casa6_3.4.0-2build1_amd64.deb ...\n",
            "Unpacking libcasa-casa6:amd64 (3.4.0-2build1) ...\n",
            "Selecting previously unselected package libcasa-python3-6:amd64.\n",
            "Preparing to unpack .../libcasa-python3-6_3.4.0-2build1_amd64.deb ...\n",
            "Unpacking libcasa-python3-6:amd64 (3.4.0-2build1) ...\n",
            "Setting up libcasa-casa6:amd64 (3.4.0-2build1) ...\n",
            "Setting up libcasa-python3-6:amd64 (3.4.0-2build1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6773a511",
      "metadata": {
        "id": "6773a511"
      },
      "source": [
        "# Setting Path to Repository\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qVvhrF3Er-Ny"
      },
      "id": "qVvhrF3Er-Ny",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "69f32376",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69f32376",
        "outputId": "c70d9032-6b91-4fca-da6a-8b2c7f1d45a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Human-Body-Measurements-using-Computer-Vision\n"
          ]
        }
      ],
      "source": [
        "%cd Human-Body-Measurements-using-Computer-Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a937a773",
      "metadata": {
        "id": "a937a773"
      },
      "source": [
        "# Defining Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1b2f52ad",
      "metadata": {
        "id": "1b2f52ad"
      },
      "outputs": [],
      "source": [
        "input_dir=r'sample_data\\input\\arsalan2.jpeg'  #Path to Test Image\n",
        "height=168  #Height in centimeters\n",
        "model_dir = 'deeplab_model' #dir to save DeepLab model (For Image Segmentation)\n",
        "pretrain='models'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b274dee",
      "metadata": {
        "id": "6b274dee"
      },
      "source": [
        "# Downloading PreTrained Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8pNYfce4cqf",
        "outputId": "b11866b7-473b-476f-cb74-aa84576f711c"
      },
      "id": "a8pNYfce4cqf",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendr\n",
            "  Downloading opendr-0.78.tar.gz (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.0/581.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from opendr) (0.29.36)\n",
            "Collecting chumpy>=0.58 (from opendr)\n",
            "  Downloading chumpy-0.70.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from opendr) (3.7.1)\n",
            "Requirement already satisfied: scipy>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from chumpy>=0.58->opendr) (1.10.1)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from chumpy>=0.58->opendr) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->opendr) (2.8.2)\n",
            "Building wheels for collected packages: opendr, chumpy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for opendr (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for opendr\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for opendr\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chumpy: filename=chumpy-0.70-py3-none-any.whl size=58264 sha256=ec5190484d0fe5cd0c6558313a6450b89f94e602fe7c4d11b39707969deba680\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/c1/ef/29ba7be03653a29ef6f2c3e1956d6c4d8877f2b243af411db1\n",
            "Successfully built chumpy\n",
            "Failed to build opendr\n",
            "\u001b[31mERROR: Could not build wheels for opendr, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOLxdq5iGbc5",
        "outputId": "551d9bc1-f57a-4fe3-efb4-fa0aed2ec309"
      },
      "id": "FOLxdq5iGbc5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=e5df087451c84409a663c71f1d171c49542499170f058ce559d55340a2b5ca71\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a281c536",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a281c536",
        "outputId": "1a3a3eac-6df0-4ce5-f2c9-ccff9f978184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading PreTrained Model\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.exists(pretrain) is False:\n",
        "  print(\"Downloading PreTrained Model\")\n",
        "\n",
        "  !python -m wget https://people.eecs.berkeley.edu/~kanazawa/cachedir/hmr/models.tar.gz && tar -xf models.tar.gz\n",
        "else:\n",
        "    print('PreTrained Model Already Downloaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be950c83",
      "metadata": {
        "id": "be950c83"
      },
      "source": [
        "# Downloading CustomBodyPoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b247d08b",
      "metadata": {
        "id": "b247d08b"
      },
      "outputs": [],
      "source": [
        "!python -m wget https://github.com/farazBhatti/Human-Body-Measurements-using-Computer-Vision/files/5886235/customBodyPoints.txt -o data/customBodyPoints.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dbff119",
      "metadata": {
        "id": "0dbff119"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.13.1-py3-none-any.whl tensorflow\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "id": "7irg5G1QQ2Pc"
      },
      "id": "7irg5G1QQ2Pc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Defines networks.\n",
        "\n",
        "@Encoder_resnet\n",
        "@Encoder_resnet_v1_101\n",
        "@Encoder_fc3_dropout\n",
        "\n",
        "@Discriminator_separable_rotations\n",
        "\n",
        "Helper:\n",
        "@get_encoder_fn_separate\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.initializers import variance_scaling\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "\n",
        "def Encoder_resnet(x, is_training=True, weight_decay=0.001, reuse=False):\n",
        "    \"\"\"\n",
        "    Resnet v2-50\n",
        "    Assumes input is [batch, height_in, width_in, channels]!!\n",
        "    Input:\n",
        "    - x: N x H x W x 3\n",
        "    - weight_decay: float\n",
        "    - reuse: bool->True if test\n",
        "\n",
        "    Outputs:\n",
        "    - cam: N x 3\n",
        "    - Pose vector: N x 72\n",
        "    - Shape vector: N x 10\n",
        "    - variables: tf variables\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.applications import ResNet50\n",
        "    with tensorflow.name_scope(\"Encoder_resnet\", [x]):\n",
        "        net = ResNet50(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_tensor=x,\n",
        "            input_shape=(None, None, 3)\n",
        "        )\n",
        "        net = tensorflow.reduce_mean(net.output, axis=[1, 2])\n",
        "    variables = net.trainable_variables if is_training else None\n",
        "    return net, variables\n",
        "\n",
        "\n",
        "def Encoder_fc3_dropout(x,\n",
        "                        num_output=85,\n",
        "                        is_training=True,\n",
        "                        reuse=False,\n",
        "                        name=\"3D_module\"):\n",
        "    \"\"\"\n",
        "    3D inference module. 3 MLP layers (last is the output)\n",
        "    With dropout  on first 2.\n",
        "    Input:\n",
        "    - x: N x [|img_feat|, |3D_param|]\n",
        "    - reuse: bool\n",
        "\n",
        "    Outputs:\n",
        "    - 3D params: N x num_output\n",
        "      if orthogonal:\n",
        "           either 85: (3 + 24*3 + 10) or 109 (3 + 24*4 + 10) for factored axis-angle representation\n",
        "      if perspective:\n",
        "          86: (f, tx, ty, tz) + 24*3 + 10, or 110 for factored axis-angle.\n",
        "    - variables: tf variables\n",
        "    \"\"\"\n",
        "    if reuse:\n",
        "        print('Reuse is on!')\n",
        "    with tensorflow.variable_scope(name, reuse=reuse):\n",
        "        net = Dense(1024, activation='relu', name='fc1')(x)\n",
        "        net = Dropout(0.5, name='dropout1')(net, training=is_training)\n",
        "        net = Dense(1024, activation='relu', name='fc2')(net)\n",
        "        net = Dropout(0.5, name='dropout2')(net, training=is_training)\n",
        "        small_xavier = VarianceScaling(scale=0.01, mode='fan_avg', distribution='uniform')\n",
        "        net = Dense(num_output, activation=None, kernel_initializer=small_xavier, name='fc3')(net)\n",
        "\n",
        "    variables = tensorflow.trainable_variables(scope=name) if is_training else None\n",
        "    return net, variables\n",
        "\n",
        "\n",
        "def get_encoder_fn_separate(model_type):\n",
        "    \"\"\"\n",
        "    Retrieves diff encoder fn for image and 3D\n",
        "    \"\"\"\n",
        "    encoder_fn = None\n",
        "    threed_fn = None\n",
        "    if 'resnet' in model_type:\n",
        "        encoder_fn = Encoder_resnet\n",
        "    else:\n",
        "        print('Unknown encoder %s!' % model_type)\n",
        "        exit(1)\n",
        "\n",
        "    if 'fc3_dropout' in model_type:\n",
        "        threed_fn = Encoder_fc3_dropout\n",
        "\n",
        "    if encoder_fn is None or threed_fn is None:\n",
        "        print('Dont know what encoder to use for %s' % model_type)\n",
        "        import ipdb\n",
        "        ipdb.set_trace()\n",
        "\n",
        "    return encoder_fn, threed_fn\n",
        "\n",
        "\n",
        "def Discriminator_separable_rotations(poses, shapes, weight_decay):\n",
        "    \"\"\"\n",
        "    23 Discriminators on each joint + 1 for all joints + 1 for shape.\n",
        "    To share the params on rotations, this treats the 23 rotation matrices\n",
        "    as a \"vertical image\":\n",
        "    Do 1x1 conv, then send off to 23 independent classifiers.\n",
        "\n",
        "    Input:\n",
        "    - poses: N x 23 x 1 x 9, NHWC ALWAYS!!\n",
        "    - shapes: N x 10\n",
        "    - weight_decay: float\n",
        "\n",
        "    Outputs:\n",
        "    - prediction: N x (1+23) or N x (1+23+1) if do_joint is on.\n",
        "    - variables: tf variables\n",
        "    \"\"\"\n",
        "    data_format = \"channels_last\"\n",
        "    with tensorflow.name_scope(\"Discriminator_sep_rotations\", [poses, shapes]):\n",
        "        with tensorflow.variable_scope(\"D\") as scope:\n",
        "            with tensorflow.keras.regularizers.l2(weight_decay):\n",
        "                poses = Conv2D(32, (1, 1), data_format=data_format, name='D_conv1')(poses)\n",
        "                poses = Conv2D(32, (1, 1), data_format=data_format, name='D_conv2')(poses)\n",
        "                theta_out = []\n",
        "                for i in range(0, 23):\n",
        "                    theta_out.append(\n",
        "                        Dense(1, activation=None, name=\"pose_out_j%d\" % i)(poses[:, i, :, :]))\n",
        "                theta_out_all = tensorflow.squeeze(tensorflow.stack(theta_out, axis=1))\n",
        "\n",
        "                # Do shape on its own:\n",
        "                shapes = Dense(5, activation='relu', kernel_regularizer=l2(weight_decay), name=\"shape_fc1\")(shapes)\n",
        "                shape_out = Dense(1, activation=None, kernel_regularizer=l2(weight_decay), name=\"shape_final\")(shapes)\n",
        "\n",
        "                \"\"\" Compute joint correlation prior!\"\"\"\n",
        "                nz_feat = 1024\n",
        "                poses_all = Flatten()(poses)\n",
        "                poses_all = Dense(nz_feat, activation='relu', kernel_regularizer=l2(weight_decay), name=\"D_alljoints_fc1\")(poses_all)\n",
        "                poses_all = Dense(nz_feat, activation='relu', kernel_regularizer=l2(weight_decay), name=\"D_alljoints_fc2\")(poses_all)\n",
        "                poses_all_out = Dense(1, activation=None, kernel_regularizer=l2(weight_decay), name=\"D_alljoints_out\")(poses_all)\n",
        "                out = tensorflow.concat([theta_out_all, poses_all_out, shape_out], 1)\n",
        "\n",
        "            variables = tensorflow.trainable_variables(scope=scope.name)\n",
        "            return out, variables\n",
        "\n"
      ],
      "metadata": {
        "id": "cciQ1d9-3Bpx"
      },
      "id": "cciQ1d9-3Bpx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Util functions implementing the camera\n",
        "\n",
        "@@batch_orth_proj_idrot\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def batch_orth_proj_idrot(X, camera, name=None):\n",
        "    \"\"\"\n",
        "    X is N x num_points x 3\n",
        "    camera is N x 3\n",
        "    same as applying orth_proj_idrot to each N\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, \"batch_orth_proj_idrot\", [X, camera]):\n",
        "        # TODO check X dim size.\n",
        "        # tf.Assert(X.shape[2] == 3, [X])\n",
        "\n",
        "        camera = tf.reshape(camera, [-1, 1, 3], name=\"cam_adj_shape\")\n",
        "\n",
        "        X_trans = X[:, :, :2] + camera[:, :, 1:]\n",
        "\n",
        "        shape = tf.shape(X_trans)\n",
        "        return tf.reshape(\n",
        "            camera[:, :, 0] * tf.reshape(X_trans, [shape[0], -1]), shape)"
      ],
      "metadata": {
        "id": "KNM6R2fy2-95"
      },
      "id": "KNM6R2fy2-95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Util functions for SMPL\n",
        "@@batch_skew\n",
        "@@batch_rodrigues\n",
        "@@batch_lrotmin\n",
        "@@batch_global_rigid_transformation\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def batch_skew(vec, batch_size=None):\n",
        "    \"\"\"\n",
        "    vec is N x 3, batch_size is int\n",
        "\n",
        "    returns N x 3 x 3. Skew_sym version of each matrix.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"batch_skew\", [vec]):\n",
        "        if batch_size is None:\n",
        "            batch_size = vec.shape.as_list()[0]\n",
        "        col_inds = tf.constant([1, 2, 3, 5, 6, 7])\n",
        "        indices = tf.reshape(\n",
        "            tf.reshape(tf.range(0, batch_size) * 9, [-1, 1]) + col_inds,\n",
        "            [-1, 1])\n",
        "        updates = tf.reshape(\n",
        "            tf.stack(\n",
        "                [\n",
        "                    -vec[:, 2], vec[:, 1], vec[:, 2], -vec[:, 0], -vec[:, 1],\n",
        "                    vec[:, 0]\n",
        "                ],\n",
        "                axis=1), [-1])\n",
        "        out_shape = [batch_size * 9]\n",
        "        res = tf.scatter_nd(indices, updates, out_shape)\n",
        "        res = tf.reshape(res, [batch_size, 3, 3])\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "def batch_rodrigues(theta, name=None):\n",
        "    \"\"\"\n",
        "    Theta is N x 3\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, \"batch_rodrigues\", [theta]):\n",
        "        batch_size = theta.shape.as_list()[0]\n",
        "\n",
        "        # angle = tf.norm(theta, axis=1)\n",
        "        # r = tf.expand_dims(tf.div(theta, tf.expand_dims(angle + 1e-8, -1)), -1)\n",
        "        # angle = tf.expand_dims(tf.norm(theta, axis=1) + 1e-8, -1)\n",
        "        angle = tf.expand_dims(tf.norm(theta + 1e-8, axis=1), -1)\n",
        "        r = tf.expand_dims(tf.div(theta, angle), -1)\n",
        "\n",
        "        angle = tf.expand_dims(angle, -1)\n",
        "        cos = tf.cos(angle)\n",
        "        sin = tf.sin(angle)\n",
        "\n",
        "        outer = tf.matmul(r, r, transpose_b=True, name=\"outer\")\n",
        "\n",
        "        eyes = tf.tile(tf.expand_dims(tf.eye(3), 0), [batch_size, 1, 1])\n",
        "        R = cos * eyes + (1 - cos) * outer + sin * batch_skew(\n",
        "            r, batch_size=batch_size)\n",
        "        return R\n",
        "\n",
        "\n",
        "def batch_lrotmin(theta, name=None):\n",
        "    \"\"\" NOTE: not used bc I want to reuse R and this is simple.\n",
        "    Output of this is used to compute joint-to-pose blend shape mapping.\n",
        "    Equation 9 in SMPL paper.\n",
        "\n",
        "\n",
        "    Args:\n",
        "      pose: `Tensor`, N x 72 vector holding the axis-angle rep of K joints.\n",
        "            This includes the global rotation so K=24\n",
        "\n",
        "    Returns\n",
        "      diff_vec : `Tensor`: N x 207 rotation matrix of 23=(K-1) joints with identity subtracted.,\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, \"batch_lrotmin\", [theta]):\n",
        "        with tf.name_scope(\"ignore_global\"):\n",
        "            theta = theta[:, 3:]\n",
        "\n",
        "        # N*23 x 3 x 3\n",
        "        Rs = batch_rodrigues(tf.reshape(theta, [-1, 3]))\n",
        "        lrotmin = tf.reshape(Rs - tf.eye(3), [-1, 207])\n",
        "\n",
        "        return lrotmin\n",
        "\n",
        "\n",
        "def batch_global_rigid_transformation(Rs, Js, parent, rotate_base=False):\n",
        "    \"\"\"\n",
        "    Computes absolute joint locations given pose.\n",
        "\n",
        "    rotate_base: if True, rotates the global rotation by 90 deg in x axis.\n",
        "    if False, this is the original SMPL coordinate.\n",
        "\n",
        "    Args:\n",
        "      Rs: N x 24 x 3 x 3 rotation vector of K joints\n",
        "      Js: N x 24 x 3, joint locations before posing\n",
        "      parent: 24 holding the parent id for each index\n",
        "\n",
        "    Returns\n",
        "      new_J : `Tensor`: N x 24 x 3 location of absolute joints\n",
        "      A     : `Tensor`: N x 24 4 x 4 relative joint transformations for LBS.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"batch_forward_kinematics\", [Rs, Js]):\n",
        "        N = Rs.shape[0].value\n",
        "        if rotate_base:\n",
        "            print('Flipping the SMPL coordinate frame!!!!')\n",
        "            rot_x = tf.constant(\n",
        "                [[1, 0, 0], [0, -1, 0], [0, 0, -1]], dtype=Rs.dtype)\n",
        "            rot_x = tf.reshape(tf.tile(rot_x, [N, 1]), [N, 3, 3])\n",
        "            root_rotation = tf.matmul(Rs[:, 0, :, :], rot_x)\n",
        "        else:\n",
        "            root_rotation = Rs[:, 0, :, :]\n",
        "\n",
        "        # Now Js is N x 24 x 3 x 1\n",
        "        Js = tf.expand_dims(Js, -1)\n",
        "\n",
        "        def make_A(R, t, name=None):\n",
        "            # Rs is N x 3 x 3, ts is N x 3 x 1\n",
        "            with tf.name_scope(name, \"Make_A\", [R, t]):\n",
        "                R_homo = tf.pad(R, [[0, 0], [0, 1], [0, 0]])\n",
        "                t_homo = tf.concat([t, tf.ones([N, 1, 1])], 1)\n",
        "                return tf.concat([R_homo, t_homo], 2)\n",
        "\n",
        "        A0 = make_A(root_rotation, Js[:, 0])\n",
        "        results = [A0]\n",
        "        for i in range(1, parent.shape[0]):\n",
        "            j_here = Js[:, i] - Js[:, parent[i]]\n",
        "            A_here = make_A(Rs[:, i], j_here)\n",
        "            res_here = tf.matmul(\n",
        "                results[parent[i]], A_here, name=\"propA%d\" % i)\n",
        "            results.append(res_here)\n",
        "\n",
        "        # 10 x 24 x 4 x 4\n",
        "        results = tf.stack(results, axis=1)\n",
        "\n",
        "        new_J = results[:, :, :3, 3]\n",
        "\n",
        "        # --- Compute relative A: Skinning is based on\n",
        "        # how much the bone moved (not the final location of the bone)\n",
        "        # but (final_bone - init_bone)\n",
        "        # ---\n",
        "        Js_w0 = tf.concat([Js, tf.zeros([N, 24, 1, 1])], 2)\n",
        "        init_bone = tf.matmul(results, Js_w0)\n",
        "        # Append empty 4 x 3:\n",
        "        init_bone = tf.pad(init_bone, [[0, 0], [0, 0], [0, 0], [3, 0]])\n",
        "        A = results - init_bone\n",
        "\n",
        "        return new_J, A\n"
      ],
      "metadata": {
        "id": "UmQ-Vwn02ZUP"
      },
      "id": "UmQ-Vwn02ZUP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tensorflow SMPL implementation as batch.\n",
        "Specify joint types:\n",
        "'coco': Returns COCO+ 19 joints\n",
        "'lsp': Returns H3.6M-LSP 14 joints\n",
        "Note: To get original smpl joints, use self.J_transformed\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pickle as pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "# from .batch_lbs import batch_rodrigues, batch_global_rigid_transformation\n",
        "\n",
        "\n",
        "# There are chumpy variables so convert them to numpy.\n",
        "def undo_chumpy(x):\n",
        "    return x if isinstance(x, np.ndarray) else x.r\n",
        "\n",
        "\n",
        "class SMPL(object):\n",
        "    def __init__(self, pkl_path, joint_type='cocoplus', dtype=tf.float32):\n",
        "        \"\"\"\n",
        "        pkl_path is the path to a SMPL model\n",
        "        \"\"\"\n",
        "        # -- Load SMPL params --\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            dd = pickle.load(f, encoding=\"latin-1\")\n",
        "        # Mean template vertices\n",
        "        self.v_template = tf.Variable(\n",
        "            undo_chumpy(dd['v_template']),\n",
        "            name='v_template',\n",
        "            dtype=dtype,\n",
        "            trainable=False)\n",
        "        # Size of mesh [Number of vertices, 3]\n",
        "        self.size = [self.v_template.shape[0].value, 3]\n",
        "        self.num_betas = dd['shapedirs'].shape[-1]\n",
        "        # Shape blend shape basis: 6980 x 3 x 10\n",
        "        # reshaped to 6980*30 x 10, transposed to 10x6980*3\n",
        "        shapedir = np.reshape(\n",
        "            undo_chumpy(dd['shapedirs']), [-1, self.num_betas]).T\n",
        "        self.shapedirs = tf.Variable(\n",
        "            shapedir, name='shapedirs', dtype=dtype, trainable=False)\n",
        "\n",
        "        # Regressor for joint locations given shape - 6890 x 24\n",
        "        self.J_regressor = tf.Variable(\n",
        "            dd['J_regressor'].T.todense(),\n",
        "            name=\"J_regressor\",\n",
        "            dtype=dtype,\n",
        "            trainable=False)\n",
        "\n",
        "        # Pose blend shape basis: 6890 x 3 x 207, reshaped to 6890*30 x 207\n",
        "        num_pose_basis = dd['posedirs'].shape[-1]\n",
        "        # 207 x 20670\n",
        "        posedirs = np.reshape(\n",
        "            undo_chumpy(dd['posedirs']), [-1, num_pose_basis]).T\n",
        "        self.posedirs = tf.Variable(\n",
        "            posedirs, name='posedirs', dtype=dtype, trainable=False)\n",
        "\n",
        "        # indices of parents for each joints\n",
        "        self.parents = dd['kintree_table'][0].astype(np.int32)\n",
        "\n",
        "        # LBS weights\n",
        "        self.weights = tf.Variable(\n",
        "            undo_chumpy(dd['weights']),\n",
        "            name='lbs_weights',\n",
        "            dtype=dtype,\n",
        "            trainable=False)\n",
        "\n",
        "        # This returns 19 keypoints: 6890 x 19\n",
        "        self.joint_regressor = tf.Variable(\n",
        "            dd['cocoplus_regressor'].T.todense(),\n",
        "            name=\"cocoplus_regressor\",\n",
        "            dtype=dtype,\n",
        "            trainable=False)\n",
        "        if joint_type == 'lsp':  # 14 LSP joints!\n",
        "            self.joint_regressor = self.joint_regressor[:, :14]\n",
        "\n",
        "        if joint_type not in ['cocoplus', 'lsp']:\n",
        "            print('BAD!! Unknown joint type: %s, it must be either \"cocoplus\" or \"lsp\"' % joint_type)\n",
        "            import ipdb\n",
        "            ipdb.set_trace()\n",
        "\n",
        "    def __call__(self, beta, theta, get_skin=False, name=None):\n",
        "        \"\"\"\n",
        "        Obtain SMPL with shape (beta) & pose (theta) inputs.\n",
        "        Theta includes the global rotation.\n",
        "        Args:\n",
        "          beta: N x 10\n",
        "          theta: N x 72 (with 3-D axis-angle rep)\n",
        "\n",
        "        Updates:\n",
        "        self.J_transformed: N x 24 x 3 joint location after shaping\n",
        "                 & posing with beta and theta\n",
        "        Returns:\n",
        "          - joints: N x 19 or 14 x 3 joint locations depending on joint_type\n",
        "        If get_skin is True, also returns\n",
        "          - Verts: N x 6980 x 3\n",
        "        \"\"\"\n",
        "\n",
        "        with tf.name_scope(name, \"smpl_main\", [beta, theta]):\n",
        "            num_batch = beta.shape[0].value\n",
        "\n",
        "            # 1. Add shape blend shapes\n",
        "            # (N x 10) x (10 x 6890*3) = N x 6890 x 3\n",
        "            v_shaped = tf.reshape(\n",
        "                tf.matmul(beta, self.shapedirs, name='shape_bs'),\n",
        "                [-1, self.size[0], self.size[1]]) + self.v_template\n",
        "\n",
        "            # 2. Infer shape-dependent joint locations.\n",
        "            Jx = tf.matmul(v_shaped[:, :, 0], self.J_regressor)\n",
        "            Jy = tf.matmul(v_shaped[:, :, 1], self.J_regressor)\n",
        "            Jz = tf.matmul(v_shaped[:, :, 2], self.J_regressor)\n",
        "            J = tf.stack([Jx, Jy, Jz], axis=2)\n",
        "\n",
        "            # 3. Add pose blend shapes\n",
        "            # N x 24 x 3 x 3\n",
        "            Rs = tf.reshape(\n",
        "                batch_rodrigues(tf.reshape(theta, [-1, 3])), [-1, 24, 3, 3])\n",
        "            with tf.name_scope(\"lrotmin\"):\n",
        "                # Ignore global rotation.\n",
        "                pose_feature = tf.reshape(Rs[:, 1:, :, :] - tf.eye(3),\n",
        "                                          [-1, 207])\n",
        "\n",
        "            # (N x 207) x (207, 20670) -> N x 6890 x 3\n",
        "            v_posed = tf.reshape(\n",
        "                tf.matmul(pose_feature, self.posedirs),\n",
        "                [-1, self.size[0], self.size[1]]) + v_shaped\n",
        "\n",
        "            #4. Get the global joint location\n",
        "            self.J_transformed, A = batch_global_rigid_transformation(Rs, J, self.parents)\n",
        "\n",
        "            # 5. Do skinning:\n",
        "            # W is N x 6890 x 24\n",
        "            W = tf.reshape(\n",
        "                tf.tile(self.weights, [num_batch, 1]), [num_batch, -1, 24])\n",
        "            # (N x 6890 x 24) x (N x 24 x 16)\n",
        "            T = tf.reshape(\n",
        "                tf.matmul(W, tf.reshape(A, [num_batch, 24, 16])),\n",
        "                [num_batch, -1, 4, 4])\n",
        "            v_posed_homo = tf.concat(\n",
        "                [v_posed, tf.ones([num_batch, v_posed.shape[1], 1])], 2)\n",
        "            v_homo = tf.matmul(T, tf.expand_dims(v_posed_homo, -1))\n",
        "\n",
        "            verts = v_homo[:, :, :3, 0]\n",
        "\n",
        "            # Get cocoplus or lsp joints:\n",
        "            joint_x = tf.matmul(verts[:, :, 0], self.joint_regressor)\n",
        "            joint_y = tf.matmul(verts[:, :, 1], self.joint_regressor)\n",
        "            joint_z = tf.matmul(verts[:, :, 2], self.joint_regressor)\n",
        "            joints = tf.stack([joint_x, joint_y, joint_z], axis=2)\n",
        "\n",
        "            if get_skin:\n",
        "                return verts, joints, Rs\n",
        "            else:\n",
        "                return joints\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GJbLTTv-Q2Ww"
      },
      "id": "GJbLTTv-Q2Ww",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Renders mesh using OpenDr for visualization.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import opendr\n",
        "from opendr.camera import ProjectPoints\n",
        "from opendr.renderer import ColoredRenderer\n",
        "from opendr.lighting import LambertianPointLight\n",
        "\n",
        "colors = {\n",
        "    # colorbline/print/copy safe:\n",
        "    'light_blue': [0.65098039, 0.74117647, 0.85882353],\n",
        "    'light_pink': [.9, .7, .7],  # This is used to do no-3d\n",
        "}\n",
        "\n",
        "\n",
        "class SMPLRenderer(object):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 flength=500.,\n",
        "                 face_path=\"tf_smpl/smpl_faces.npy\"):\n",
        "        self.faces = np.load(face_path)\n",
        "        self.w = img_size\n",
        "        self.h = img_size\n",
        "        self.flength = flength\n",
        "\n",
        "    def __call__(self,\n",
        "                 verts,\n",
        "                 cam=None,\n",
        "                 img=None,\n",
        "                 do_alpha=False,\n",
        "                 far=None,\n",
        "                 near=None,\n",
        "                 color_id=0,\n",
        "                 img_size=None):\n",
        "        \"\"\"\n",
        "        cam is 3D [f, px, py]\n",
        "        \"\"\"\n",
        "        if img is not None:\n",
        "            h, w = img.shape[:2]\n",
        "        elif img_size is not None:\n",
        "            h = img_size[0]\n",
        "            w = img_size[1]\n",
        "        else:\n",
        "            h = self.h\n",
        "            w = self.w\n",
        "\n",
        "        if cam is None:\n",
        "            cam = [self.flength, w / 2., h / 2.]\n",
        "\n",
        "        use_cam = ProjectPoints(\n",
        "            f=cam[0] * np.ones(2),\n",
        "            rt=np.zeros(3),\n",
        "            t=np.zeros(3),\n",
        "            k=np.zeros(5),\n",
        "            c=cam[1:3])\n",
        "\n",
        "        if near is None:\n",
        "            near = np.maximum(np.min(verts[:, 2]) - 25, 0.1)\n",
        "        if far is None:\n",
        "            far = np.maximum(np.max(verts[:, 2]) + 25, 25)\n",
        "\n",
        "        imtmp = render_model(\n",
        "            verts,\n",
        "            self.faces,\n",
        "            w,\n",
        "            h,\n",
        "            use_cam,\n",
        "            do_alpha=do_alpha,\n",
        "            img=img,\n",
        "            far=far,\n",
        "            near=near,\n",
        "            color_id=color_id)\n",
        "\n",
        "        return (imtmp * 255).astype('uint8')\n",
        "\n",
        "    def rotated(self,\n",
        "                verts,\n",
        "                deg,\n",
        "                cam=None,\n",
        "                axis='y',\n",
        "                img=None,\n",
        "                do_alpha=True,\n",
        "                far=None,\n",
        "                near=None,\n",
        "                color_id=0,\n",
        "                img_size=None):\n",
        "        import math\n",
        "        if axis == 'y':\n",
        "            around = cv2.Rodrigues(np.array([0, math.radians(deg), 0]))[0]\n",
        "        elif axis == 'x':\n",
        "            around = cv2.Rodrigues(np.array([math.radians(deg), 0, 0]))[0]\n",
        "        else:\n",
        "            around = cv2.Rodrigues(np.array([0, 0, math.radians(deg)]))[0]\n",
        "        center = verts.mean(axis=0)\n",
        "        new_v = np.dot((verts - center), around) + center\n",
        "\n",
        "        return self.__call__(\n",
        "            new_v,\n",
        "            cam,\n",
        "            img=img,\n",
        "            do_alpha=do_alpha,\n",
        "            far=far,\n",
        "            near=near,\n",
        "            img_size=img_size,\n",
        "            color_id=color_id)\n",
        "\n",
        "\n",
        "def _create_renderer(w=640,\n",
        "                     h=480,\n",
        "                     rt=np.zeros(3),\n",
        "                     t=np.zeros(3),\n",
        "                     f=None,\n",
        "                     c=None,\n",
        "                     k=None,\n",
        "                     near=.5,\n",
        "                     far=10.):\n",
        "\n",
        "    f = np.array([w, w]) / 2. if f is None else f\n",
        "    c = np.array([w, h]) / 2. if c is None else c\n",
        "    k = np.zeros(5) if k is None else k\n",
        "\n",
        "    rn = ColoredRenderer()\n",
        "\n",
        "    rn.camera = ProjectPoints(rt=rt, t=t, f=f, c=c, k=k)\n",
        "    rn.frustum = {'near': near, 'far': far, 'height': h, 'width': w}\n",
        "    return rn\n",
        "\n",
        "\n",
        "def _rotateY(points, angle):\n",
        "    \"\"\"Rotate the points by a specified angle.\"\"\"\n",
        "    ry = np.array([[np.cos(angle), 0., np.sin(angle)], [0., 1., 0.],\n",
        "                   [-np.sin(angle), 0., np.cos(angle)]])\n",
        "    return np.dot(points, ry)\n",
        "\n",
        "\n",
        "def simple_renderer(rn,\n",
        "                    verts,\n",
        "                    faces,\n",
        "                    yrot=np.radians(120),\n",
        "                    color=colors['light_pink']):\n",
        "    # Rendered model color\n",
        "    rn.set(v=verts, f=faces, vc=color, bgcolor=np.ones(3))\n",
        "    albedo = rn.vc\n",
        "\n",
        "    # Construct Back Light (on back right corner)\n",
        "    rn.vc = LambertianPointLight(\n",
        "        f=rn.f,\n",
        "        v=rn.v,\n",
        "        num_verts=len(rn.v),\n",
        "        light_pos=_rotateY(np.array([-200, -100, -100]), yrot),\n",
        "        vc=albedo,\n",
        "        light_color=np.array([1, 1, 1]))\n",
        "\n",
        "    # Construct Left Light\n",
        "    rn.vc += LambertianPointLight(\n",
        "        f=rn.f,\n",
        "        v=rn.v,\n",
        "        num_verts=len(rn.v),\n",
        "        light_pos=_rotateY(np.array([800, 10, 300]), yrot),\n",
        "        vc=albedo,\n",
        "        light_color=np.array([1, 1, 1]))\n",
        "\n",
        "    # Construct Right Light\n",
        "    rn.vc += LambertianPointLight(\n",
        "        f=rn.f,\n",
        "        v=rn.v,\n",
        "        num_verts=len(rn.v),\n",
        "        light_pos=_rotateY(np.array([-500, 500, 1000]), yrot),\n",
        "        vc=albedo,\n",
        "        light_color=np.array([.7, .7, .7]))\n",
        "\n",
        "    return rn.r\n",
        "\n",
        "\n",
        "def get_alpha(imtmp, bgval=1.):\n",
        "    h, w = imtmp.shape[:2]\n",
        "    alpha = (~np.all(imtmp == bgval, axis=2)).astype(imtmp.dtype)\n",
        "\n",
        "    b_channel, g_channel, r_channel = cv2.split(imtmp)\n",
        "\n",
        "    im_RGBA = cv2.merge((b_channel, g_channel, r_channel, alpha.astype(\n",
        "        imtmp.dtype)))\n",
        "    return im_RGBA\n",
        "\n",
        "\n",
        "def append_alpha(imtmp):\n",
        "    alpha = np.ones_like(imtmp[:, :, 0]).astype(imtmp.dtype)\n",
        "    if np.issubdtype(imtmp.dtype, np.uint8):\n",
        "        alpha = alpha * 255\n",
        "    b_channel, g_channel, r_channel = cv2.split(imtmp)\n",
        "    im_RGBA = cv2.merge((b_channel, g_channel, r_channel, alpha))\n",
        "    return im_RGBA\n",
        "\n",
        "\n",
        "def render_model(verts,\n",
        "                 faces,\n",
        "                 w,\n",
        "                 h,\n",
        "                 cam,\n",
        "                 near=0.5,\n",
        "                 far=25,\n",
        "                 img=None,\n",
        "                 do_alpha=False,\n",
        "                 color_id=None):\n",
        "    rn = _create_renderer(\n",
        "        w=w, h=h, near=near, far=far, rt=cam.rt, t=cam.t, f=cam.f, c=cam.c)\n",
        "\n",
        "    # Uses img as background, otherwise white background.\n",
        "    if img is not None:\n",
        "        rn.background_image = img / 255. if img.max() > 1 else img\n",
        "\n",
        "    if color_id is None:\n",
        "        color = colors['light_blue']\n",
        "    else:\n",
        "        color_list = list(colors.values())\n",
        "        color = color_list[color_id % len(color_list)]\n",
        "\n",
        "    imtmp = simple_renderer(rn, verts, faces, color=color)\n",
        "\n",
        "    # If white bg, make transparent.\n",
        "    if img is None and do_alpha:\n",
        "        imtmp = get_alpha(imtmp)\n",
        "    elif img is not None and do_alpha:\n",
        "        imtmp = append_alpha(imtmp)\n",
        "\n",
        "    return imtmp\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "\n",
        "\n",
        "def get_original(proc_param, verts, cam, joints, img_size):\n",
        "    img_size = proc_param['img_size']\n",
        "    undo_scale = 1. / np.array(proc_param['scale'])\n",
        "\n",
        "    cam_s = cam[0]\n",
        "    cam_pos = cam[1:]\n",
        "    principal_pt = np.array([img_size, img_size]) / 2.\n",
        "    flength = 500.\n",
        "    tz = flength / (0.5 * img_size * cam_s)\n",
        "    trans = np.hstack([cam_pos, tz])\n",
        "    vert_shifted = verts + trans\n",
        "\n",
        "    start_pt = proc_param['start_pt'] - 0.5 * img_size\n",
        "    final_principal_pt = (principal_pt + start_pt) * undo_scale\n",
        "    cam_for_render = np.hstack(\n",
        "        [np.mean(flength * undo_scale), final_principal_pt])\n",
        "\n",
        "    # This is in padded image.\n",
        "    # kp_original = (joints + proc_param['start_pt']) * undo_scale\n",
        "    # Subtract padding from joints.\n",
        "    margin = int(img_size / 2)\n",
        "    kp_original = (joints + proc_param['start_pt'] - margin) * undo_scale\n",
        "\n",
        "    return cam_for_render, vert_shifted, kp_original\n",
        "\n",
        "\n",
        "def draw_skeleton(input_image, joints, draw_edges=True, vis=None, radius=None):\n",
        "    \"\"\"\n",
        "    joints is 3 x 19. but if not will transpose it.\n",
        "    0: Right ankle\n",
        "    1: Right knee\n",
        "    2: Right hip\n",
        "    3: Left hip\n",
        "    4: Left knee\n",
        "    5: Left ankle\n",
        "    6: Right wrist\n",
        "    7: Right elbow\n",
        "    8: Right shoulder\n",
        "    9: Left shoulder\n",
        "    10: Left elbow\n",
        "    11: Left wrist\n",
        "    12: Neck\n",
        "    13: Head top\n",
        "    14: nose\n",
        "    15: left_eye\n",
        "    16: right_eye\n",
        "    17: left_ear\n",
        "    18: right_ear\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "\n",
        "    if radius is None:\n",
        "        radius = max(4, (np.mean(input_image.shape[:2]) * 0.01).astype(int))\n",
        "\n",
        "    colors = {\n",
        "        'pink': np.array([197, 27, 125]),  # L lower leg\n",
        "        'light_pink': np.array([233, 163, 201]),  # L upper leg\n",
        "        'light_green': np.array([161, 215, 106]),  # L lower arm\n",
        "        'green': np.array([77, 146, 33]),  # L upper arm\n",
        "        'red': np.array([215, 48, 39]),  # head\n",
        "        'light_red': np.array([252, 146, 114]),  # head\n",
        "        'light_orange': np.array([252, 141, 89]),  # chest\n",
        "        'purple': np.array([118, 42, 131]),  # R lower leg\n",
        "        'light_purple': np.array([175, 141, 195]),  # R upper\n",
        "        'light_blue': np.array([145, 191, 219]),  # R lower arm\n",
        "        'blue': np.array([69, 117, 180]),  # R upper arm\n",
        "        'gray': np.array([130, 130, 130]),  #\n",
        "        'white': np.array([255, 255, 255]),  #\n",
        "    }\n",
        "\n",
        "    image = input_image.copy()\n",
        "    input_is_float = False\n",
        "\n",
        "    if np.issubdtype(image.dtype, np.float):\n",
        "        input_is_float = True\n",
        "        max_val = image.max()\n",
        "        if max_val <= 2.:  # should be 1 but sometimes it's slightly above 1\n",
        "            image = (image * 255).astype(np.uint8)\n",
        "        else:\n",
        "            image = (image).astype(np.uint8)\n",
        "\n",
        "    if joints.shape[0] != 2:\n",
        "        joints = joints.T\n",
        "    joints = np.round(joints).astype(int)\n",
        "\n",
        "    jcolors = [\n",
        "        'light_pink', 'light_pink', 'light_pink', 'pink', 'pink', 'pink',\n",
        "        'light_blue', 'light_blue', 'light_blue', 'blue', 'blue', 'blue',\n",
        "        'purple', 'purple', 'red', 'green', 'green', 'white', 'white'\n",
        "    ]\n",
        "\n",
        "    if joints.shape[1] == 19:\n",
        "        # parent indices -1 means no parents\n",
        "        parents = np.array([\n",
        "            1, 2, 8, 9, 3, 4, 7, 8, 12, 12, 9, 10, 14, -1, 13, -1, -1, 15, 16\n",
        "        ])\n",
        "        # Left is light and right is dark\n",
        "        ecolors = {\n",
        "            0: 'light_pink',\n",
        "            1: 'light_pink',\n",
        "            2: 'light_pink',\n",
        "            3: 'pink',\n",
        "            4: 'pink',\n",
        "            5: 'pink',\n",
        "            6: 'light_blue',\n",
        "            7: 'light_blue',\n",
        "            8: 'light_blue',\n",
        "            9: 'blue',\n",
        "            10: 'blue',\n",
        "            11: 'blue',\n",
        "            12: 'purple',\n",
        "            17: 'light_green',\n",
        "            18: 'light_green',\n",
        "            14: 'purple'\n",
        "        }\n",
        "    elif joints.shape[1] == 19:\n",
        "        parents = np.array([\n",
        "            1,\n",
        "            2,\n",
        "            8,\n",
        "            9,\n",
        "            3,\n",
        "            4,\n",
        "            7,\n",
        "            8,\n",
        "            -1,\n",
        "            -1,\n",
        "            9,\n",
        "            10,\n",
        "            13,\n",
        "            -1,\n",
        "        ])\n",
        "        ecolors = {\n",
        "            0: 'light_pink',\n",
        "            1: 'light_pink',\n",
        "            2: 'light_pink',\n",
        "            3: 'pink',\n",
        "            4: 'pink',\n",
        "            5: 'pink',\n",
        "            6: 'light_blue',\n",
        "            7: 'light_blue',\n",
        "            10: 'light_blue',\n",
        "            11: 'blue',\n",
        "            12: 'purple'\n",
        "        }\n",
        "    else:\n",
        "        print('Unknown skeleton!!')\n",
        "        import ipdb\n",
        "        ipdb.set_trace()\n",
        "\n",
        "    for child in range(len(parents)):\n",
        "        point = joints[:, child]\n",
        "        # If invisible skip\n",
        "        if vis is not None and vis[child] == 0:\n",
        "            continue\n",
        "        if draw_edges:\n",
        "            cv2.circle(image, (point[0], point[1]), radius, tuple([int(x) for x in colors['white']]),-1)#colors['white']\n",
        "            cv2.circle(image, (point[0], point[1]), radius - 1,\n",
        "                       tuple([int(x) for x in colors[jcolors[child]]]), -1)\n",
        "        else:\n",
        "            # cv2.circle(image, (point[0], point[1]), 5, colors['white'], 1)\n",
        "            cv2.circle(image, (point[0], point[1]), radius - 1,\n",
        "                       colors[jcolors[child]], 1)\n",
        "            # cv2.circle(image, (point[0], point[1]), 5, colors['gray'], -1)\n",
        "        pa_id = parents[child]\n",
        "        if draw_edges and pa_id >= 0:\n",
        "            if vis is not None and vis[pa_id] == 0:\n",
        "                continue\n",
        "            point_pa = joints[:, pa_id]\n",
        "            cv2.circle(image, (point_pa[0], point_pa[1]), radius - 1,\n",
        "                       tuple([int(x) for x in colors[jcolors[pa_id]]]), -1)\n",
        "            if child not in ecolors.keys():\n",
        "                print('bad')\n",
        "                import ipdb\n",
        "                ipdb.set_trace()\n",
        "            cv2.line(image, (point[0], point[1]), (point_pa[0], point_pa[1]),\n",
        "                     tuple([int(x) for x in colors[ecolors[child]]]), radius - 2)\n",
        "\n",
        "    # Convert back in original dtype\n",
        "    if input_is_float:\n",
        "        if max_val <= 1.:\n",
        "            image = image.astype(np.float32) / 255.\n",
        "        else:\n",
        "            image = image.astype(np.float32)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_text(input_image, content):\n",
        "    \"\"\"\n",
        "    content is a dict. draws key: val on image\n",
        "    Assumes key is str, val is float\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import cv2\n",
        "    image = input_image.copy()\n",
        "    input_is_float = False\n",
        "    if np.issubdtype(image.dtype, np.float):\n",
        "        input_is_float = True\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    black = np.array([0, 0, 0])\n",
        "    margin = 15\n",
        "    start_x = 5\n",
        "    start_y = margin\n",
        "    for key in sorted(content.keys()):\n",
        "        text = \"%s: %.2g\" % (key, content[key])\n",
        "        cv2.putText(image, text, (start_x, start_y), 0, 0.45, black)\n",
        "        start_y += margin\n",
        "\n",
        "    if input_is_float:\n",
        "        image = image.astype(np.float32) / 255.\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "7z-vAOLf4Kc5"
      },
      "id": "7z-vAOLf4Kc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03979707",
      "metadata": {
        "id": "03979707"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import os\n",
        "from io import BytesIO\n",
        "from absl import flags\n",
        "\n",
        "# import src.config\n",
        "## remove the log_dir flag from config.py\n",
        "import sys\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from six.moves import urllib\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2, pdb, glob, argparse\n",
        "import tensorflow as tensorflow\n",
        "\n",
        "from os.path import exists\n",
        "\n",
        "# from src.tensorflow_smpl import projection as proj_util\n",
        "\n",
        "\n",
        "# from src.models import get_encoder_fn_separate\n",
        "## tensorflow.contrib.slim migration change in models.py file\n",
        "\n",
        "import extract_measurements\n",
        "import cv2\n",
        "import skimage.io as io\n",
        "\n",
        "from src.util import renderer as vis_util\n",
        "from src.util import image as img_util\n",
        "from src.util import openpose as op_util\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01b3385",
      "metadata": {
        "id": "a01b3385"
      },
      "source": [
        "# Defining Helper Functions and DeepLab model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60e679b",
      "metadata": {
        "id": "a60e679b"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DeepLabModel(object):\n",
        "\t\"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "\tINPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "\tOUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
        "\tINPUT_SIZE = 513\n",
        "\tFROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "\tdef __init__(self, tarball_path):\n",
        "\t\t#\"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "\t\tself.graph = tensorflow.Graph()\n",
        "\t\tgraph_def = None\n",
        "\t\t# Extract frozen graph from tar archive.\n",
        "\t\ttar_file = tarfile.open(tarball_path)\n",
        "\t\tfor tar_info in tar_file.getmembers():\n",
        "\t\t\tif self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "\t\t\t\tfile_handle = tar_file.extractensorflowile(tar_info)\n",
        "\t\t\t\tgraph_def = tensorflow.GraphDef.FromString(file_handle.read())\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\ttar_file.close()\n",
        "\n",
        "\t\tif graph_def is None:\n",
        "\t\t\traise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "\t\twith self.graph.as_default():\n",
        "\t\t\ttensorflow.import_graph_def(graph_def, name='')\n",
        "\n",
        "\t\tself.sess = tensorflow.Session(graph=self.graph)\n",
        "\n",
        "\tdef run(self, image):\n",
        "\t\t\"\"\"Runs inference on a single image.\n",
        "\n",
        "\t\tArgs:\n",
        "\t\t  image: A PIL.Image object, raw input image.\n",
        "\n",
        "\t\tReturns:\n",
        "\t\t  resized_image: RGB image resized from original input image.\n",
        "\t\t  seg_map: Segmentation map of `resized_image`.\n",
        "\t\t\"\"\"\n",
        "\t\twidth, height = image.size\n",
        "\t\tresize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
        "\t\ttarget_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
        "\t\tresized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "\t\tbatch_seg_map = self.sess.run(\n",
        "\t\t\tself.OUTPUT_TENSOR_NAME,\n",
        "\t\t\tfeed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "\t\tseg_map = batch_seg_map[0]\n",
        "\t\treturn resized_image, seg_map\n",
        "\n",
        "def create_pascal_label_colormap():\n",
        "\t\"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
        "\n",
        "\tReturns:\n",
        "\tA Colormap for visualizing segmentation results.\n",
        "\t\"\"\"\n",
        "\tcolormap = np.zeros((256, 3), dtype=int)\n",
        "\tind = np.arange(256, dtype=int)\n",
        "\n",
        "\tfor shift in reversed(range(8)):\n",
        "\t\tfor channel in range(3):\n",
        "\t\t  colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
        "\t\tind >>= 3\n",
        "\n",
        "\treturn colormap\n",
        "\n",
        "def label_to_color_image(label):\n",
        "\t\"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "\tArgs:\n",
        "\tlabel: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "\tReturns:\n",
        "\tresult: A 2D array with floating type. The element of the array\n",
        "\t  is the color indexed by the corresponding element in the input label\n",
        "\t  to the PASCAL color map.\n",
        "\n",
        "\tRaises:\n",
        "\tValueError: If label is not of rank 2 or its value is larger than color\n",
        "\t  map maximum entry.\n",
        "\t\"\"\"\n",
        "\tif label.ndim != 2:\n",
        "\t\traise ValueError('Expect 2-D input label')\n",
        "\n",
        "\tcolormap = create_pascal_label_colormap()\n",
        "\n",
        "\tif np.max(label) >= len(colormap):\n",
        "\t\traise ValueError('label value too large.')\n",
        "\n",
        "\treturn colormap[label]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_image(img_path, json_path=None):\n",
        "    img = img_path#io.imread(img_path)\n",
        "    print(\"$$$$$$$\",img.shape)\n",
        "    if img.shape[2] == 4:\n",
        "        img = img[:, :, :3]\n",
        "\n",
        "    if json_path is None:\n",
        "        if np.max(img.shape[:2]) != 224:\n",
        "#            print('Resizing so the max image size is %d..' % config.img_size)\n",
        "            scale = (float(224) / np.max(img.shape[:2]))\n",
        "        else:\n",
        "            scale = 1.\n",
        "        center = np.round(np.array(img.shape[:2]) / 2).astype(int)\n",
        "        # image center in (x,y)\n",
        "        center = center[::-1]\n",
        "    else:\n",
        "        scale, center = op_util.get_bbox(json_path)\n",
        "\n",
        "    crop, proc_param = img_util.scale_and_crop(img, scale, center,\n",
        "                                               224)\n",
        "\n",
        "    # Normalize image to [-1, 1]\n",
        "    crop = 2 * ((crop / 255.) - 0.5)\n",
        "\n",
        "    return crop, proc_param, img\n",
        "\n",
        "\n",
        "def main(img_path, height, json_path=None):\n",
        "#    renderer = vis_util.SMPLRenderer(face_path='src/tensorflow_smpl/smpl_faces.npy')\n",
        "    sess = tensorflow.Session()\n",
        "    model = RunModel(sess=sess)\n",
        "#    cv2.imshow('input image for measurement extraction',img_path)\n",
        "#    cv2.waitKey(0)\n",
        "\n",
        "    input_img, proc_param, img = preprocess_image(img_path, json_path)\n",
        "    # Add batch dimension: 1 x D x D x 3\n",
        "    input_img = np.expand_dims(input_img, 0)\n",
        "\n",
        "    # Theta is the 85D vector holding [camera, pose, shape]\n",
        "    # where camera is 3D [s, tx, ty]\n",
        "    # pose is 72D vector holding the rotation of 24 joints of SMPL in axis angle format\n",
        "    # shape is 10D shape coefficients of SMPL\n",
        "    joints, verts, cams, joints3d, theta = model.predict(\n",
        "        input_img, get_theta=True)\n",
        "\n",
        "    extract_measurements.extract_measurements(height,verts[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6d0530",
      "metadata": {
        "id": "da6d0530"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-prYFhA_19kt"
      },
      "id": "-prYFhA_19kt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e808710a",
      "metadata": {
        "id": "e808710a"
      },
      "outputs": [],
      "source": [
        "class RunModel(object):\n",
        "    def __init__(self, sess=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          config\n",
        "        \"\"\"\n",
        "#        self.config = config\n",
        "\n",
        "        self.load_path = 'models/model.ckpt-667589'#config.load_path\n",
        "\n",
        "\n",
        "        # Data\n",
        "        self.batch_size = 1#config.batch_size\n",
        "        self.img_size = 224#config.img_size\n",
        "\n",
        "\n",
        "        self.data_format = 'NHMC'\n",
        "        self.smpl_model_path = 'models/neutral_smpl_with_cocoplus_reg.pkl'#config.smpl_model_path\n",
        "\n",
        "        input_size = (self.batch_size, self.img_size, self.img_size, 3)\n",
        "        self.images_pl = tensorflow.placeholder(tensorflow.float32, shape=input_size)\n",
        "\n",
        "        # Model Settings\n",
        "        self.num_stage = 3#config.num_stage\n",
        "        self.model_type = 'resnet_fc3_dropout'#config.model_type\n",
        "\n",
        "        self.joint_type = 'cocoplus'#config.joint_type\n",
        "\n",
        "        # Camera\n",
        "        self.num_cam = 3\n",
        "        self.proj_fn = batch_orth_proj_idrot\n",
        "\n",
        "        self.num_theta = 72\n",
        "        # Theta size: camera (3) + pose (24*3) + shape (10)\n",
        "        self.total_params = self.num_cam + self.num_theta + 10\n",
        "\n",
        "        self.smpl = SMPL(self.smpl_model_path, joint_type=self.joint_type)\n",
        "\n",
        "\n",
        "\n",
        "        self.build_test_model_ief()\n",
        "\n",
        "        if sess is None:\n",
        "            self.sess = tensorflow.Session()\n",
        "        else:\n",
        "            self.sess = sess\n",
        "\n",
        "        # Load data.\n",
        "        self.saver = tensorflow.train.Saver()\n",
        "        self.prepare()\n",
        "\n",
        "\n",
        "    def build_test_model_ief(self):\n",
        "        # Load mean value\n",
        "        self.mean_var = tensorflow.Variable(tensorflow.zeros((1, self.total_params)), name=\"mean_param\", dtype=tensorflow.float32)\n",
        "\n",
        "        img_enc_fn, threed_enc_fn = get_encoder_fn_separate(self.model_type)\n",
        "        # Extract image features.\n",
        "        self.img_feat, self.E_var = img_enc_fn(self.images_pl,\n",
        "                                               is_training=False,\n",
        "                                               reuse=False)\n",
        "\n",
        "        # Start loop\n",
        "        self.all_verts = []\n",
        "        self.all_kps = []\n",
        "        self.all_cams = []\n",
        "        self.all_Js = []\n",
        "        self.final_thetas = []\n",
        "        theta_prev = tensorflow.tile(self.mean_var, [self.batch_size, 1])\n",
        "        for i in np.arange(self.num_stage):\n",
        "            print('Iteration %d' % i)\n",
        "            # ---- Compute outputs\n",
        "            state = tensorflow.concat([self.img_feat, theta_prev], 1)\n",
        "\n",
        "            if i == 0:\n",
        "                delta_theta, _ = threed_enc_fn(\n",
        "                    state,\n",
        "                    num_output=self.total_params,\n",
        "                    is_training=False,\n",
        "                    reuse=False)\n",
        "            else:\n",
        "                delta_theta, _ = threed_enc_fn(\n",
        "                    state,\n",
        "                    num_output=self.total_params,\n",
        "                    is_training=False,\n",
        "                    reuse=True)\n",
        "\n",
        "            # Compute new theta\n",
        "            theta_here = theta_prev + delta_theta\n",
        "            # cam = N x 3, pose N x self.num_theta, shape: N x 10\n",
        "            cams = theta_here[:, :self.num_cam]\n",
        "            poses = theta_here[:, self.num_cam:(self.num_cam + self.num_theta)]\n",
        "            shapes = theta_here[:, (self.num_cam + self.num_theta):]\n",
        "\n",
        "            verts, Js, _ = self.smpl(shapes, poses, get_skin=True)\n",
        "\n",
        "            # Project to 2D!\n",
        "            pred_kp = self.proj_fn(Js, cams, name='proj_2d_stage%d' % i)\n",
        "            self.all_verts.append(verts)\n",
        "            self.all_kps.append(pred_kp)\n",
        "            self.all_cams.append(cams)\n",
        "            self.all_Js.append(Js)\n",
        "            # save each theta.\n",
        "            self.final_thetas.append(theta_here)\n",
        "            # Finally)update to end iteration.\n",
        "            theta_prev = theta_here\n",
        "\n",
        "\n",
        "    def prepare(self):\n",
        "        print('Restoring checkpoint %s..' % self.load_path)\n",
        "        self.saver.restore(self.sess, self.load_path)\n",
        "        self.mean_value = self.sess.run(self.mean_var)\n",
        "\n",
        "    def predict(self, images, get_theta=False):\n",
        "        \"\"\"\n",
        "        images: num_batch, img_size, img_size, 3\n",
        "        Preprocessed to range [-1, 1]\n",
        "        \"\"\"\n",
        "        results = self.predict_dict(images)\n",
        "        if get_theta:\n",
        "            return results['joints'], results['verts'], results['cams'], results[\n",
        "                'joints3d'], results['theta']\n",
        "        else:\n",
        "            return results['joints'], results['verts'], results['cams'], results[\n",
        "                'joints3d']\n",
        "\n",
        "    def predict_dict(self, images):\n",
        "        \"\"\"\n",
        "        images: num_batch, img_size, img_size, 3\n",
        "        Preprocessed to range [-1, 1]\n",
        "        Runs the model with images.\n",
        "        \"\"\"\n",
        "        feed_dict = {\n",
        "            self.images_pl: images,\n",
        "            # self.theta0_pl: self.mean_var,\n",
        "        }\n",
        "        fetch_dict = {\n",
        "            'joints': self.all_kps[-1],\n",
        "            'verts': self.all_verts[-1],\n",
        "            'cams': self.all_cams[-1],\n",
        "            'joints3d': self.all_Js[-1],\n",
        "            'theta': self.final_thetas[-1],\n",
        "        }\n",
        "\n",
        "        results = self.sess.run(fetch_dict, feed_dict)\n",
        "\n",
        "        # Return joints in original image space.\n",
        "        joints = results['joints']\n",
        "        results['joints'] = ((joints + 1) * 0.5) * self.img_size\n",
        "\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efe49d3",
      "metadata": {
        "id": "9efe49d3"
      },
      "source": [
        "# Inference Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a929887",
      "metadata": {
        "id": "1a929887"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "dir_name=input_dir;\n",
        "\n",
        "\n",
        "## setup ####################\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "\t'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "\t'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
        "\t'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
        "])\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n",
        "\n",
        "\n",
        "MODEL_NAME = 'mobilenetv2_coco_voctrainval'  # @param ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "\t'mobilenetv2_coco_voctrainaug':\n",
        "\t\t'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',\n",
        "\t'mobilenetv2_coco_voctrainval':\n",
        "\t\t'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',\n",
        "\t'xception_coco_voctrainaug':\n",
        "\t\t'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',\n",
        "\t'xception_coco_voctrainval':\n",
        "\t\t'deeplabv3_pascal_trainval_2018_01_04.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = _MODEL_URLS[MODEL_NAME]\n",
        "\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "  tensorflow.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "if not os.path.exists(download_path):\n",
        "  print('downloading model to %s, this might take a while...' % download_path)\n",
        "  urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n",
        "\t\t\t     download_path)\n",
        "  print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')\n",
        "\n",
        "\n",
        "image = Image.open(dir_name)\n",
        "#print(\"Image Type = \",type(image))\n",
        "back = cv2.imread('sample_data/input/background.jpeg',cv2.IMREAD_COLOR)\n",
        "\n",
        "\n",
        "res_im,seg=MODEL.run(image)\n",
        "\n",
        "seg=cv2.resize(seg.astype(np.uint8),image.size)\n",
        "mask_sel=(seg==15).astype(np.float32)\n",
        "mask = 255*mask_sel.astype(np.uint8)\n",
        "\n",
        "img = \tnp.array(image)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "res = cv2.bitwise_and(img,img,mask = mask)\n",
        "bg_removed = res + (255 - cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR))\n",
        "main(bg_removed,height,None)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9cc6ee",
      "metadata": {
        "id": "9b9cc6ee"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}